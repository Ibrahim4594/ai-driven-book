"use strict";(globalThis.webpackChunkai_driven_book=globalThis.webpackChunkai_driven_book||[]).push([[2354],{2675:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>l,default:()=>m,frontMatter:()=>d,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"chapters/03-rag-chatbots","title":"Chapter 7: Building RAG-Based Chatbots","description":"What is RAG (Retrieval-Augmented Generation)?","source":"@site/docs/chapters/03-rag-chatbots.md","sourceDirName":"chapters","slug":"/chapters/03-rag-chatbots","permalink":"/ai-driven-book/docs/chapters/03-rag-chatbots","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"id":"03-rag-chatbots","slug":"/chapters/03-rag-chatbots","title":"Chapter 7: Building RAG-Based Chatbots"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Spec-Driven Methodology","permalink":"/ai-driven-book/docs/chapters/spec-driven"},"next":{"title":"Chapter 8: Implementation Guide","permalink":"/ai-driven-book/docs/chapters/implementation"}}');var s=n(4848),r=n(8453),o=n(8440),a=n(7010),c=n(4442);const d={sidebar_position:7,id:"03-rag-chatbots",slug:"/chapters/03-rag-chatbots",title:"Chapter 7: Building RAG-Based Chatbots"},l="Chapter 7: Building RAG-Based Chatbots",u={},p=[{value:"What is RAG (Retrieval-Augmented Generation)?",id:"what-is-rag-retrieval-augmented-generation",level:2},{value:"The RAG Architecture",id:"the-rag-architecture",level:3},{value:"Why RAG?",id:"why-rag",level:2},{value:"Advantages Over Fine-Tuning",id:"advantages-over-fine-tuning",level:3},{value:"Use Cases",id:"use-cases",level:3},{value:"Vector Databases",id:"vector-databases",level:2},{value:"What Are Embeddings?",id:"what-are-embeddings",level:3},{value:"Popular Vector Databases",id:"popular-vector-databases",level:3},{value:"Qdrant Cloud Setup",id:"qdrant-cloud-setup",level:3},{value:"Document Processing Pipeline",id:"document-processing-pipeline",level:2},{value:"Step 1: Chunk Documents",id:"step-1-chunk-documents",level:3},{value:"Step 2: Generate Embeddings",id:"step-2-generate-embeddings",level:3},{value:"Step 3: Store in Vector Database",id:"step-3-store-in-vector-database",level:3},{value:"Retrieval Strategy",id:"retrieval-strategy",level:2},{value:"Basic Semantic Search",id:"basic-semantic-search",level:3},{value:"Hybrid Search",id:"hybrid-search",level:3},{value:"OpenAI Agents SDK",id:"openai-agents-sdk",level:2},{value:"Creating a RAG Agent",id:"creating-a-rag-agent",level:3},{value:"Running the Agent",id:"running-the-agent",level:3},{value:"FastAPI Backend",id:"fastapi-backend",level:2},{value:"Project Structure",id:"project-structure",level:3},{value:"Main Application",id:"main-application",level:3},{value:"Chat Endpoint",id:"chat-endpoint",level:3},{value:"RAG Service",id:"rag-service",level:3},{value:"Summary",id:"summary",level:2},{value:"\ud83c\udfb4 Test Your Knowledge",id:"-test-your-knowledge",level:2},{value:"\ud83d\udcdd Chapter Quiz",id:"-chapter-quiz",level:2}];function h(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"chapter-7-building-rag-based-chatbots",children:"Chapter 7: Building RAG-Based Chatbots"})}),"\n",(0,s.jsx)("div",{style:{textAlign:"center",margin:"3rem 0"},children:(0,s.jsx)("img",{src:"/ai-driven-book/img/undraw-ai-chat.svg",alt:"RAG Chatbots",style:{maxWidth:"460px",width:"100%",height:"auto",filter:"drop-shadow(0 4px 12px rgba(0, 102, 255, 0.15))"}})}),"\n",(0,s.jsx)(t.h2,{id:"what-is-rag-retrieval-augmented-generation",children:"What is RAG (Retrieval-Augmented Generation)?"}),"\n",(0,s.jsx)(t.p,{children:"RAG combines the power of large language models with external knowledge retrieval to create chatbots that can answer questions based on specific documents or data sources."}),"\n",(0,s.jsx)(t.h3,{id:"the-rag-architecture",children:"The RAG Architecture"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"User Query \u2192 Embedding \u2192 Vector Search \u2192 Context Retrieval \u2192 LLM \u2192 Response\n"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Components:"})}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Document Ingestion"}),": Convert documents to embeddings"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Vector Database"}),": Store and search embeddings efficiently"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Retrieval"}),": Find relevant context for queries"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Generation"}),": LLM generates answers using retrieved context"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Response"}),": Return formatted answer to user"]}),"\n"]}),"\n","\n",(0,s.jsx)(o.A,{title:"RAG Architecture Flow",diagram:o.f.ragArchitecture,caption:"Visual representation of how RAG systems process user queries through embedding, retrieval, and generation stages."}),"\n",(0,s.jsx)(o.A,{title:"RAG Workflow Sequence",diagram:o.f.ragWorkflow,caption:"Step-by-step sequence showing the interaction between different components in a RAG system."}),"\n",(0,s.jsx)(t.h2,{id:"why-rag",children:"Why RAG?"}),"\n",(0,s.jsx)(t.h3,{id:"advantages-over-fine-tuning",children:"Advantages Over Fine-Tuning"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"No retraining"})," required for new information"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Transparent sources"})," - know where answers come from"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Easy updates"})," - just add new documents"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Cost-effective"})," - no GPU training time"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Reduced hallucinations"})," - grounded in actual documents"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"use-cases",children:"Use Cases"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Customer support chatbots"}),"\n",(0,s.jsx)(t.li,{children:"Documentation assistants"}),"\n",(0,s.jsx)(t.li,{children:"Internal knowledge bases"}),"\n",(0,s.jsx)(t.li,{children:"Research assistants"}),"\n",(0,s.jsx)(t.li,{children:"Educational tutors"}),"\n",(0,s.jsx)(t.li,{children:"Legal document analysis"}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"vector-databases",children:"Vector Databases"}),"\n",(0,s.jsx)(t.h3,{id:"what-are-embeddings",children:"What Are Embeddings?"}),"\n",(0,s.jsx)(t.p,{children:"Embeddings are numerical representations of text that capture semantic meaning:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'# Text\n"AI-driven development is transforming software engineering"\n\n# Embedding (simplified)\n[0.23, -0.45, 0.67, ..., 0.12]  # 1536 dimensions for OpenAI\'s text-embedding-3-small\n'})}),"\n",(0,s.jsx)(t.p,{children:"Similar texts have similar embeddings, enabling semantic search."}),"\n",(0,s.jsx)(t.h3,{id:"popular-vector-databases",children:"Popular Vector Databases"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Qdrant"})," - Fast, scalable, open-source"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Pinecone"})," - Managed service, easy setup"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Weaviate"})," - GraphQL interface"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Milvus"})," - Large-scale deployments"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"ChromaDB"})," - Lightweight, embedded"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"qdrant-cloud-setup",children:"Qdrant Cloud Setup"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\n# Connect to Qdrant Cloud\nclient = QdrantClient(\n    url="https://your-cluster.qdrant.io",\n    api_key="your-api-key"\n)\n\n# Create collection\nclient.create_collection(\n    collection_name="book_knowledge",\n    vectors_config=VectorParams(\n        size=1536,  # OpenAI embedding dimension\n        distance=Distance.COSINE\n    )\n)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"document-processing-pipeline",children:"Document Processing Pipeline"}),"\n",(0,s.jsx)(t.h3,{id:"step-1-chunk-documents",children:"Step 1: Chunk Documents"}),"\n",(0,s.jsx)(t.p,{children:"Break large documents into smaller, semantically meaningful chunks:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def chunk_document(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n    """Split document into overlapping chunks"""\n    chunks = []\n    start = 0\n\n    while start < len(text):\n        end = start + chunk_size\n        chunk = text[start:end]\n\n        # Try to break at sentence boundary\n        if end < len(text):\n            last_period = chunk.rfind(\'.\')\n            if last_period > chunk_size * 0.7:  # At least 70% of chunk size\n                end = start + last_period + 1\n                chunk = text[start:end]\n\n        chunks.append(chunk.strip())\n        start = end - overlap  # Overlap for context continuity\n\n    return chunks\n'})}),"\n",(0,s.jsx)(t.h3,{id:"step-2-generate-embeddings",children:"Step 2: Generate Embeddings"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(api_key="your-key")\n\ndef get_embedding(text: str) -> list[float]:\n    """Get embedding for text"""\n    response = client.embeddings.create(\n        model="text-embedding-3-small",\n        input=text\n    )\n    return response.data[0].embedding\n'})}),"\n",(0,s.jsx)(t.h3,{id:"step-3-store-in-vector-database",children:"Step 3: Store in Vector Database"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from qdrant_client.models import PointStruct\nimport uuid\n\ndef ingest_document(document: str, metadata: dict):\n    """Process and store document in Qdrant"""\n    chunks = chunk_document(document)\n\n    points = []\n    for i, chunk in enumerate(chunks):\n        embedding = get_embedding(chunk)\n\n        point = PointStruct(\n            id=str(uuid.uuid4()),\n            vector=embedding,\n            payload={\n                "text": chunk,\n                "chunk_index": i,\n                **metadata\n            }\n        )\n        points.append(point)\n\n    client.upsert(\n        collection_name="book_knowledge",\n        points=points\n    )\n'})}),"\n",(0,s.jsx)(t.h2,{id:"retrieval-strategy",children:"Retrieval Strategy"}),"\n",(0,s.jsx)(t.h3,{id:"basic-semantic-search",children:"Basic Semantic Search"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def search_knowledge(query: str, top_k: int = 5) -> list[dict]:\n    """Search for relevant context"""\n    query_embedding = get_embedding(query)\n\n    results = client.search(\n        collection_name="book_knowledge",\n        query_vector=query_embedding,\n        limit=top_k\n    )\n\n    return [\n        {\n            "text": hit.payload["text"],\n            "score": hit.score,\n            "metadata": hit.payload\n        }\n        for hit in results\n    ]\n'})}),"\n",(0,s.jsx)(t.h3,{id:"hybrid-search",children:"Hybrid Search"}),"\n",(0,s.jsx)(t.p,{children:"Combine semantic search with keyword matching:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def hybrid_search(query: str, top_k: int = 5) -> list[dict]:\n    """Combine vector and keyword search"""\n    query_embedding = get_embedding(query)\n\n    results = client.search(\n        collection_name="book_knowledge",\n        query_vector=query_embedding,\n        query_filter={\n            "must": [\n                {\n                    "key": "text",\n                    "match": {\n                        "text": query\n                    }\n                }\n            ]\n        },\n        limit=top_k\n    )\n\n    return [{"text": hit.payload["text"], "score": hit.score} for hit in results]\n'})}),"\n",(0,s.jsx)(t.h2,{id:"openai-agents-sdk",children:"OpenAI Agents SDK"}),"\n",(0,s.jsx)(t.h3,{id:"creating-a-rag-agent",children:"Creating a RAG Agent"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-typescript",children:"import { Agent } from '@openai/agents-sdk';\nimport { QdrantClient } from '@qdrant/js-client-rest';\n\nconst agent = new Agent({\n  model: 'gpt-4o',\n  instructions: `You are a helpful assistant that answers questions about\n  AI-Driven Development. Use the provided context to answer questions accurately.\n  If you don't know the answer based on the context, say so.`,\n  tools: [\n    {\n      type: 'function',\n      function: {\n        name: 'search_knowledge',\n        description: 'Search the knowledge base for relevant information',\n        parameters: {\n          type: 'object',\n          properties: {\n            query: {\n              type: 'string',\n              description: 'The search query'\n            }\n          },\n          required: ['query']\n        }\n      }\n    }\n  ]\n});\n\n// Handle tool calls\nagent.on('tool_call', async (toolCall) => {\n  if (toolCall.function.name === 'search_knowledge') {\n    const { query } = JSON.parse(toolCall.function.arguments);\n    const results = await searchKnowledge(query);\n\n    return {\n      tool_call_id: toolCall.id,\n      output: JSON.stringify(results)\n    };\n  }\n});\n"})}),"\n",(0,s.jsx)(t.h3,{id:"running-the-agent",children:"Running the Agent"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-typescript",children:"const response = await agent.run({\n  messages: [\n    { role: 'user', content: 'What are the benefits of spec-driven development?' }\n  ]\n});\n\nconsole.log(response.content);\n"})}),"\n",(0,s.jsx)(t.h2,{id:"fastapi-backend",children:"FastAPI Backend"}),"\n",(0,s.jsx)(t.h3,{id:"project-structure",children:"Project Structure"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"backend/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 models.py\n\u2502   \u251c\u2500\u2500 routes/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 chat.py\n\u2502   \u2502   \u2514\u2500\u2500 documents.py\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 embeddings.py\n\u2502   \u2502   \u251c\u2500\u2500 qdrant.py\n\u2502   \u2502   \u2514\u2500\u2500 rag.py\n\u2502   \u2514\u2500\u2500 config.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 Dockerfile\n"})}),"\n",(0,s.jsx)(t.h3,{id:"main-application",children:"Main Application"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'# app/main.py\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom app.routes import chat, documents\nfrom app.config import settings\n\napp = FastAPI(\n    title="RAG Chatbot API",\n    description="API for AI-Driven Development book chatbot",\n    version="1.0.0"\n)\n\n# CORS configuration\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=["*"],  # Configure for production\n    allow_credentials=True,\n    allow_methods=["*"],\n    allow_headers=["*"],\n)\n\n# Include routers\napp.include_router(chat.router, prefix="/api/chat", tags=["chat"])\napp.include_router(documents.router, prefix="/api/documents", tags=["documents"])\n\n@app.get("/")\nasync def root():\n    return {"message": "RAG Chatbot API", "status": "running"}\n'})}),"\n",(0,s.jsx)(t.h3,{id:"chat-endpoint",children:"Chat Endpoint"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'# app/routes/chat.py\nfrom fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom app.services.rag import RAGService\n\nrouter = APIRouter()\nrag_service = RAGService()\n\nclass ChatRequest(BaseModel):\n    message: str\n    context: str | None = None  # Optional selected text context\n    conversation_id: str | None = None\n\nclass ChatResponse(BaseModel):\n    response: str\n    sources: list[dict]\n    conversation_id: str\n\n@router.post("/", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    """Handle chat message with RAG"""\n    try:\n        result = await rag_service.generate_response(\n            query=request.message,\n            context=request.context,\n            conversation_id=request.conversation_id\n        )\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n'})}),"\n",(0,s.jsx)(t.h3,{id:"rag-service",children:"RAG Service"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'# app/services/rag.py\nfrom openai import AsyncOpenAI\nfrom qdrant_client import AsyncQdrantClient\nfrom app.config import settings\n\nclass RAGService:\n    def __init__(self):\n        self.openai = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)\n        self.qdrant = AsyncQdrantClient(\n            url=settings.QDRANT_URL,\n            api_key=settings.QDRANT_API_KEY\n        )\n\n    async def generate_response(\n        self,\n        query: str,\n        context: str | None = None,\n        conversation_id: str | None = None\n    ) -> dict:\n        """Generate RAG response"""\n\n        # If context provided, search within it\n        if context:\n            relevant_chunks = await self._search_in_context(query, context)\n        else:\n            # Search entire knowledge base\n            relevant_chunks = await self._search_knowledge(query)\n\n        # Build prompt with context\n        context_text = "\\n\\n".join([\n            f"[Source {i+1}]: {chunk[\'text\']}"\n            for i, chunk in enumerate(relevant_chunks)\n        ])\n\n        # Generate response\n        response = await self.openai.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {\n                    "role": "system",\n                    "content": f"""You are a helpful assistant for the AI-Driven\n                    Development book. Answer questions based on the provided context.\n                    Cite sources when appropriate.\n\n                    Context:\n                    {context_text}\n                    """\n                },\n                {\n                    "role": "user",\n                    "content": query\n                }\n            ]\n        )\n\n        return {\n            "response": response.choices[0].message.content,\n            "sources": [\n                {\n                    "text": chunk["text"][:200] + "...",\n                    "score": chunk["score"]\n                }\n                for chunk in relevant_chunks\n            ],\n            "conversation_id": conversation_id or "new"\n        }\n\n    async def _search_knowledge(self, query: str, top_k: int = 5) -> list[dict]:\n        """Search vector database"""\n        query_embedding = await self._get_embedding(query)\n\n        results = await self.qdrant.search(\n            collection_name=settings.QDRANT_COLLECTION,\n            query_vector=query_embedding,\n            limit=top_k\n        )\n\n        return [\n            {\n                "text": hit.payload["text"],\n                "score": hit.score\n            }\n            for hit in results\n        ]\n\n    async def _search_in_context(self, query: str, context: str) -> list[dict]:\n        """Search within provided context"""\n        # Chunk the context\n        chunks = self._chunk_text(context)\n\n        # Get embeddings for query and chunks\n        query_embedding = await self._get_embedding(query)\n        chunk_embeddings = [await self._get_embedding(chunk) for chunk in chunks]\n\n        # Calculate similarity scores\n        scores = [\n            self._cosine_similarity(query_embedding, chunk_emb)\n            for chunk_emb in chunk_embeddings\n        ]\n\n        # Return top chunks\n        sorted_chunks = sorted(\n            zip(chunks, scores),\n            key=lambda x: x[1],\n            reverse=True\n        )[:3]\n\n        return [\n            {"text": chunk, "score": score}\n            for chunk, score in sorted_chunks\n        ]\n\n    async def _get_embedding(self, text: str) -> list[float]:\n        """Get text embedding"""\n        response = await self.openai.embeddings.create(\n            model="text-embedding-3-small",\n            input=text\n        )\n        return response.data[0].embedding\n\n    def _chunk_text(self, text: str, size: int = 500) -> list[str]:\n        """Split text into chunks"""\n        words = text.split()\n        chunks = []\n        for i in range(0, len(words), size):\n            chunk = " ".join(words[i:i + size])\n            chunks.append(chunk)\n        return chunks\n\n    def _cosine_similarity(self, a: list[float], b: list[float]) -> float:\n        """Calculate cosine similarity"""\n        import math\n        dot_product = sum(x * y for x, y in zip(a, b))\n        magnitude_a = math.sqrt(sum(x * x for x in a))\n        magnitude_b = math.sqrt(sum(y * y for y in b))\n        return dot_product / (magnitude_a * magnitude_b)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(t.p,{children:"In this chapter, you learned:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"RAG architecture and components"}),"\n",(0,s.jsx)(t.li,{children:"Vector databases and embeddings"}),"\n",(0,s.jsx)(t.li,{children:"Document processing pipelines"}),"\n",(0,s.jsx)(t.li,{children:"OpenAI Agents SDK"}),"\n",(0,s.jsx)(t.li,{children:"FastAPI backend implementation"}),"\n",(0,s.jsx)(t.li,{children:"Context-aware search functionality"}),"\n"]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"-test-your-knowledge",children:"\ud83c\udfb4 Test Your Knowledge"}),"\n","\n",(0,s.jsx)(a.A,{cards:a.d.ch3,title:"Chapter 3: RAG Chatbots Flashcards"}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"-chapter-quiz",children:"\ud83d\udcdd Chapter Quiz"}),"\n","\n",(0,s.jsx)(c.A,{questions:c.f.ch7,title:"Chapter 7 Quiz"}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Next Chapter:"})," We'll integrate the chatbot into the Docusaurus frontend and implement text selection features."]})]})}function m(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},4442:(e,t,n)=>{n.d(t,{f:()=>a,A:()=>o});var i=n(6540);const s={mcqContainer:"mcqContainer_aMzE",header:"header_ufij",subtitle:"subtitle_A_xb",scoreCard:"scoreCard_FrpM",scoreBig:"scoreBig_LQ0Z",scorePercentage:"scorePercentage_sCU0",scoreLabel:"scoreLabel_rRZo",questionsList:"questionsList_umTx",questionCard:"questionCard_Wfqx",questionHeader:"questionHeader_UsZJ",questionNumber:"questionNumber_Eicx",correctBadge:"correctBadge_exPB",incorrectBadge:"incorrectBadge__psw",questionText:"questionText_kqM1",optionsList:"optionsList_dCUU",option:"option_z5CR",selected:"selected_UDZP",correct:"correct_wiey",incorrect:"incorrect_zD8a",showCorrect:"showCorrect_uKRT",optionLetter:"optionLetter_H8LO",optionText:"optionText_GLrF",correctIcon:"correctIcon_ruBo",explanation:"explanation_u46W",explanationLabel:"explanationLabel_vf_h",explanationText:"explanationText_t2cW",submitButton:"submitButton_aQgt",actions:"actions_fsw7",submitAllButton:"submitAllButton_V4VJ",retryButton:"retryButton_LySZ"};var r=n(4848);function o({questions:e,title:t="Chapter Quiz"}){const[n,o]=(0,i.useState)(new Array(e.length).fill(-1)),[a,c]=(0,i.useState)(new Array(e.length).fill(!1)),[d,l]=(0,i.useState)(0),[u,p]=(0,i.useState)(!1),h=n.every(e=>-1!==e),m=a.every(e=>e);return(0,r.jsxs)("div",{className:s.mcqContainer,children:[(0,r.jsxs)("div",{className:s.header,children:[(0,r.jsxs)("h3",{children:["\ud83d\udcdd ",t]}),(0,r.jsx)("p",{className:s.subtitle,children:"Test your understanding with these multiple choice questions"}),u&&(0,r.jsxs)("div",{className:s.scoreCard,children:[(0,r.jsxs)("div",{className:s.scoreBig,children:[d,"/",e.length]}),(0,r.jsxs)("div",{className:s.scorePercentage,children:[Math.round(d/e.length*100),"%"]}),(0,r.jsx)("div",{className:s.scoreLabel,children:d===e.length?"\ud83c\udf89 Perfect Score!":d>=.7*e.length?"\u2705 Great Job!":d>=.5*e.length?"\ud83d\udc4d Good Effort!":"\ud83d\udcda Keep Learning!"})]})]}),(0,r.jsx)("div",{className:s.questionsList,children:e.map((t,i)=>(0,r.jsxs)("div",{className:s.questionCard,children:[(0,r.jsxs)("div",{className:s.questionHeader,children:[(0,r.jsxs)("span",{className:s.questionNumber,children:["Question ",i+1]}),a[i]&&(0,r.jsx)("span",{className:t.options[n[i]]?.isCorrect?s.correctBadge:s.incorrectBadge,children:t.options[n[i]]?.isCorrect?"\u2713 Correct":"\u2717 Incorrect"})]}),(0,r.jsx)("div",{className:s.questionText,children:t.question}),(0,r.jsx)("div",{className:s.optionsList,children:t.options.map((e,t)=>{const c=n[i]===t,d=a[i],l=e.isCorrect;let u=s.option;return c&&(u+=` ${s.selected}`),d&&c&&l&&(u+=` ${s.correct}`),d&&c&&!l&&(u+=` ${s.incorrect}`),d&&!c&&l&&(u+=` ${s.showCorrect}`),(0,r.jsxs)("button",{className:u,onClick:()=>((e,t)=>{if(a[e])return;const i=[...n];i[e]=t,o(i)})(i,t),disabled:a[i],children:[(0,r.jsx)("span",{className:s.optionLetter,children:String.fromCharCode(65+t)}),(0,r.jsx)("span",{className:s.optionText,children:e.text}),d&&l&&(0,r.jsx)("span",{className:s.correctIcon,children:"\u2713"})]},t)})}),a[i]&&(0,r.jsxs)("div",{className:s.explanation,children:[(0,r.jsx)("div",{className:s.explanationLabel,children:"\ud83d\udca1 Explanation:"}),(0,r.jsx)("div",{className:s.explanationText,children:t.explanation})]}),!a[i]&&-1!==n[i]&&(0,r.jsx)("button",{className:s.submitButton,onClick:()=>(t=>{const i=[...a];i[t]=!0,c(i);const s=e[t].options[n[t]]?.isCorrect;s&&l(d+1)})(i),children:"Submit Answer"})]},i))}),(0,r.jsxs)("div",{className:s.actions,children:[!m&&(0,r.jsx)("button",{className:s.submitAllButton,onClick:()=>{let t=0;const i=e.map(()=>!0);e.forEach((e,i)=>{-1!==n[i]&&e.options[n[i]].isCorrect&&t++}),c(i),l(t),p(!0)},disabled:!h,children:h?"Submit All Answers":`Answer All Questions (${n.filter(e=>-1!==e).length}/${e.length})`}),m&&(0,r.jsx)("button",{className:s.retryButton,onClick:()=>{o(new Array(e.length).fill(-1)),c(new Array(e.length).fill(!1)),l(0),p(!1)},children:"\ud83d\udd04 Retry Quiz"})]})]})}const a={ch1:[{question:"What is the primary function of the transformer architecture in Large Language Models?",options:[{text:"To compress data for storage",isCorrect:!1},{text:"To use self-attention mechanisms to understand context",isCorrect:!0},{text:"To convert images to text",isCorrect:!1},{text:"To optimize database queries",isCorrect:!1}],explanation:"Transformers use self-attention mechanisms to understand relationships between words in context, allowing LLMs to generate coherent and contextually relevant text."},{question:"What are tokens in the context of LLMs?",options:[{text:"Security credentials for API access",isCorrect:!1},{text:"Basic units of text that LLMs process (words, subwords, or characters)",isCorrect:!0},{text:"Currency used to pay for AI services",isCorrect:!1},{text:"Encryption keys for data security",isCorrect:!1}],explanation:"Tokens are the fundamental units LLMs process. Text is broken down into tokens (which can be words, parts of words, or characters) before being processed by the model."},{question:"What is the difference between fine-tuning and prompt engineering?",options:[{text:"Fine-tuning is cheaper and faster than prompt engineering",isCorrect:!1},{text:"Prompt engineering requires retraining the entire model",isCorrect:!1},{text:"Fine-tuning retrains the model on specific data, while prompt engineering crafts effective instructions",isCorrect:!0},{text:"They are the same thing with different names",isCorrect:!1}],explanation:"Fine-tuning involves retraining an LLM on specialized data (costly and time-consuming), while prompt engineering guides the model through well-crafted instructions without retraining."},{question:"What is a context window in LLMs?",options:[{text:"A graphical user interface for AI tools",isCorrect:!1},{text:"The maximum amount of text an LLM can consider at once",isCorrect:!0},{text:"The time period during which AI generates responses",isCorrect:!1},{text:"The physical window on your screen displaying AI output",isCorrect:!1}],explanation:"A context window is the maximum amount of text (measured in tokens) that an LLM can process simultaneously. Larger context windows allow better understanding of longer documents but increase computational costs."},{question:"Why is AI-driven development transforming software engineering?",options:[{text:"It completely replaces human developers",isCorrect:!1},{text:"It accelerates coding, automates tasks, and allows developers to focus on problem-solving",isCorrect:!0},{text:"It eliminates the need for testing and debugging",isCorrect:!1},{text:"It only works for simple applications",isCorrect:!1}],explanation:"AI-driven development enhances productivity by automating routine tasks, generating code from specifications, and assisting with debugging, allowing developers to focus on high-level design and complex problem-solving."}],ch2:[{question:"What is the main difference between traditional development and AI-driven development?",options:[{text:"AI-driven development doesn't require any coding skills",isCorrect:!1},{text:"AI-driven development uses AI as a pair programmer to assist with coding tasks",isCorrect:!0},{text:"Traditional development is always faster",isCorrect:!1},{text:"AI-driven development can only be used for web applications",isCorrect:!1}],explanation:"AI-driven development leverages AI tools as intelligent assistants that help with code generation, debugging, testing, and documentation, while traditional development relies entirely on manual coding by humans."},{question:"Which of the following is NOT a benefit of AI coding assistants?",options:[{text:"Faster code generation from natural language",isCorrect:!1},{text:"Automated bug detection and testing",isCorrect:!1},{text:"Guaranteed bug-free code every time",isCorrect:!0},{text:"Code review and refactoring suggestions",isCorrect:!1}],explanation:"AI assistants are powerful but not perfect. They can help detect bugs and improve code quality, but they don't guarantee bug-free code. Human review and testing are still essential."},{question:"What is a common pitfall when using AI coding assistants?",options:[{text:"They are too slow to be practical",isCorrect:!1},{text:"Over-relying on AI without understanding the generated code",isCorrect:!0},{text:"They only work with Python",isCorrect:!1},{text:"They require expensive hardware",isCorrect:!1}],explanation:"A major pitfall is blindly accepting AI-generated code without understanding it. This can lead to security vulnerabilities, bugs, and maintainability issues. Always review and test AI-generated code."},{question:"How should developers integrate AI tools into their workflow?",options:[{text:"Use AI to write all code and never review it",isCorrect:!1},{text:"Only use AI for documentation, never for actual code",isCorrect:!1},{text:"Use AI for initial drafts and boilerplate, then review and refine",isCorrect:!0},{text:"Replace all testing with AI-generated tests",isCorrect:!1}],explanation:"Effective integration involves using AI for initial code generation and boilerplate, then applying human expertise to review, test, and refine the output. This combines AI efficiency with human judgment."},{question:"What role does prompt engineering play in AI-driven development?",options:[{text:"It's only useful for chatbots",isCorrect:!1},{text:"It helps developers craft clear instructions to get better AI-generated code",isCorrect:!0},{text:"It's a waste of time compared to traditional coding",isCorrect:!1},{text:"It only works with specific programming languages",isCorrect:!1}],explanation:"Prompt engineering is crucial for AI-driven development. Well-crafted prompts with clear requirements, context, and examples lead to more accurate, maintainable, and useful AI-generated code."}],ch3:[{question:"Which type of AI tool provides inline code suggestions directly in your IDE?",options:[{text:"Chat-based assistants like ChatGPT",isCorrect:!1},{text:"IDE-integrated tools like GitHub Copilot",isCorrect:!0},{text:"CLI tools like Aider",isCorrect:!1},{text:"Specialized platforms like Devin AI",isCorrect:!1}],explanation:"IDE-integrated tools like GitHub Copilot, Cursor, and Tabnine provide real-time inline code suggestions as you type, seamlessly integrating with your development environment."},{question:"What is the main advantage of cloud-based AI models over local models?",options:[{text:"They work without internet connection",isCorrect:!1},{text:"They are completely free to use",isCorrect:!1},{text:"They offer superior performance and regular updates",isCorrect:!0},{text:"They guarantee complete data privacy",isCorrect:!1}],explanation:"Cloud-based models (GPT-4, Claude) provide state-of-the-art performance, large context windows, and regular updates. However, they require internet connectivity and raise privacy considerations."},{question:"When choosing an AI coding tool, which factor is MOST important for large codebases?",options:[{text:"The color scheme of the interface",isCorrect:!1},{text:"Context window size and codebase understanding",isCorrect:!0},{text:"The number of programming languages supported",isCorrect:!1},{text:"The speed of simple autocomplete",isCorrect:!1}],explanation:"For large codebases, context window size is crucial. Tools with larger context windows can understand more of your codebase at once, leading to more relevant and accurate suggestions."},{question:"What distinguishes GitHub Copilot from ChatGPT for coding tasks?",options:[{text:"Copilot requires no API key while ChatGPT does",isCorrect:!1},{text:"Copilot provides inline suggestions in your IDE, ChatGPT offers conversational help",isCorrect:!0},{text:"ChatGPT is always more accurate than Copilot",isCorrect:!1},{text:"Copilot only works with JavaScript",isCorrect:!1}],explanation:"GitHub Copilot integrates directly into your IDE for inline suggestions as you type, while ChatGPT provides conversational assistance for explanations, problem-solving, and refactoring through a chat interface."},{question:"Which AI tool category is best for terminal-based workflows?",options:[{text:"IDE plugins",isCorrect:!1},{text:"Web-based chatbots",isCorrect:!1},{text:"CLI tools like Claude Code or Aider",isCorrect:!0},{text:"Specialized autonomous platforms",isCorrect:!1}],explanation:"CLI tools like Claude Code and Aider are specifically designed for command-line workflows, allowing developers who prefer terminal-based development to leverage AI assistance."}],ch4:[{question:"What is the most important principle of effective prompt engineering?",options:[{text:"Use as few words as possible",isCorrect:!1},{text:"Be specific and provide clear context with examples",isCorrect:!0},{text:"Always use technical jargon",isCorrect:!1},{text:"Never mention edge cases",isCorrect:!1}],explanation:"Effective prompts are specific, provide clear context, include examples, and define constraints. Clarity and detail lead to more accurate AI responses."},{question:"How does Markdown improve prompts for AI coding assistants?",options:[{text:"It makes prompts look prettier but doesn't affect results",isCorrect:!1},{text:"It provides structure through headings, lists, and code blocks",isCorrect:!0},{text:"It encrypts sensitive information in prompts",isCorrect:!1},{text:"It reduces the token count of prompts",isCorrect:!1}],explanation:"Markdown provides structure that makes prompts more readable for both humans and AI. Headings, lists, code blocks, and emphasis help organize requirements and highlight important information."},{question:"What should you include in the context of a prompt?",options:[{text:"Only the immediate task with no background",isCorrect:!1},{text:"Project tech stack, coding standards, existing patterns, and requirements",isCorrect:!0},{text:"Personal information about team members",isCorrect:!1},{text:"Unrelated code from different projects",isCorrect:!1}],explanation:"Good context includes relevant information about your tech stack, coding standards, existing code patterns, user requirements, and constraints. More relevant context leads to better AI suggestions."},{question:"What should you do when AI generates incorrect or suboptimal code?",options:[{text:"Give up and write all code manually",isCorrect:!1},{text:"Use the code anyway without changes",isCorrect:!1},{text:"Refine your prompt with more details and iterate",isCorrect:!0},{text:"Switch to a different programming language",isCorrect:!1}],explanation:"When AI output is poor, refine your prompt with more specific details, provide examples of desired output, point out issues, and iterate. Use AI as a collaborative tool, not a one-shot solution."},{question:"Why is it important to break complex tasks into smaller steps in prompts?",options:[{text:"To confuse the AI model",isCorrect:!1},{text:"To make prompts longer and more impressive",isCorrect:!1},{text:"AI performs better with focused, manageable tasks",isCorrect:!0},{text:"It's not important; always use one giant prompt",isCorrect:!1}],explanation:"Breaking complex tasks into smaller, focused steps helps AI understand each component better, produces more accurate results, and makes debugging easier when issues arise."}],ch5:[{question:"What is the core principle of Specification-Driven Development (SDD)?",options:[{text:"Write code first, then document it",isCorrect:!1},{text:"Write detailed specifications before implementation",isCorrect:!0},{text:"Skip documentation entirely",isCorrect:!1},{text:"Only write specifications for complex features",isCorrect:!1}],explanation:"SDD emphasizes creating clear, detailed specifications before writing any code. These specs serve as contracts between requirements and implementation, especially powerful with AI code generation."},{question:"Which is NOT an essential element of a good specification?",options:[{text:"Context explaining the 'why'",isCorrect:!1},{text:"Functional requirements with details",isCorrect:!1},{text:"The developer's personal preferences",isCorrect:!0},{text:"Success criteria and acceptance tests",isCorrect:!1}],explanation:"A complete specification includes context, objectives, functional requirements, data models, edge cases, non-functional requirements, and success criteria. Personal preferences are not part of specifications."},{question:"How does SDD improve AI-generated code quality?",options:[{text:"It doesn't; AI works the same with or without specs",isCorrect:!1},{text:"It provides comprehensive context and clear acceptance criteria for validation",isCorrect:!0},{text:"It makes the AI generate code faster",isCorrect:!1},{text:"It reduces the need for testing",isCorrect:!1}],explanation:"Detailed specifications provide AI with comprehensive context, clear interfaces, explicit edge cases, and validation criteria, resulting in more accurate and maintainable code."},{question:"What makes SDD specifications different from traditional waterfall documentation?",options:[{text:"SDD specs are never updated after creation",isCorrect:!1},{text:"SDD specs are living documents, executable, and designed for AI consumption",isCorrect:!0},{text:"SDD specs are shorter and less detailed",isCorrect:!1},{text:"There is no difference",isCorrect:!1}],explanation:"Unlike static waterfall docs, SDD specifications are living documents that are continuously updated, executable/testable, and optimized for AI tools to generate code from them."},{question:"What should specifications include regarding edge cases?",options:[{text:"Nothing; edge cases can be figured out during implementation",isCorrect:!1},{text:"Explicit definitions of edge cases and how to handle them",isCorrect:!0},{text:"Only the most common edge case",isCorrect:!1},{text:"A note saying 'handle edge cases appropriately'",isCorrect:!1}],explanation:"Good specifications explicitly define edge cases and specify exactly how they should be handled. This prevents ambiguity and ensures AI-generated code properly addresses all scenarios."}],ch6:[{question:"In Spec-Driven Methodology, when should specifications be reviewed?",options:[{text:"Only after all code is written",isCorrect:!1},{text:"Before AI generates code from them",isCorrect:!0},{text:"Never; specifications don't need review",isCorrect:!1},{text:"Only when bugs are found",isCorrect:!1}],explanation:"Specifications should be reviewed with stakeholders before code generation to ensure alignment, catch issues early, and provide a solid foundation for AI-assisted implementation."},{question:"How should specifications be structured for AI to understand them effectively?",options:[{text:"As free-form text with no structure",isCorrect:!1},{text:"Using structured formats like Markdown or YAML with explicit data types",isCorrect:!0},{text:"As images or diagrams only",isCorrect:!1},{text:"In natural language without any technical details",isCorrect:!1}],explanation:"AI-friendly specs use structured formats (Markdown, YAML), are explicit about data types and constraints, include examples, and avoid ambiguity through consistent terminology."},{question:"Which tool is commonly used for API specifications in Spec-Driven Development?",options:[{text:"Microsoft Word",isCorrect:!1},{text:"OpenAPI/Swagger",isCorrect:!0},{text:"Adobe Photoshop",isCorrect:!1},{text:"Notepad",isCorrect:!1}],explanation:"OpenAPI (formerly Swagger) is the industry standard for API specifications, providing a structured way to define endpoints, request/response formats, and validation rules."},{question:"How do you validate that AI-generated code meets specifications?",options:[{text:"Just trust that the AI got it right",isCorrect:!1},{text:"Use automated tests derived from spec acceptance criteria",isCorrect:!0},{text:"Only check if it compiles",isCorrect:!1},{text:"Validation isn't necessary with AI-generated code",isCorrect:!1}],explanation:"Validation requires automated tests based on acceptance criteria, type checking, API contract testing, code review, and integration tests for edge cases specified in the spec."},{question:"What role do specifications play after code is generated?",options:[{text:"They should be deleted to save space",isCorrect:!1},{text:"They serve as living documentation and are updated as requirements change",isCorrect:!0},{text:"They are archived and never looked at again",isCorrect:!1},{text:"They are only useful during initial development",isCorrect:!1}],explanation:"Specifications remain valuable as living documentation, single source of truth, basis for automated testing, and guides for future changes. They should be updated as requirements evolve."}],ch7:[{question:"What does RAG stand for in AI systems?",options:[{text:"Random Access Generation",isCorrect:!1},{text:"Retrieval-Augmented Generation",isCorrect:!0},{text:"Rapid Application Generator",isCorrect:!1},{text:"Recursive Algorithm Gateway",isCorrect:!1}],explanation:"RAG stands for Retrieval-Augmented Generation. It combines information retrieval (searching documents) with AI generation to provide accurate, context-aware responses based on specific data."},{question:"How do vector embeddings enable semantic search in RAG?",options:[{text:"They compress files to save storage space",isCorrect:!1},{text:"They convert text to numerical vectors that capture meaning",isCorrect:!0},{text:"They translate text between languages",isCorrect:!1},{text:"They encrypt data for security",isCorrect:!1}],explanation:"Vector embeddings convert text into high-dimensional numerical vectors that capture semantic meaning. Similar texts have similar vectors, enabling similarity search for relevant documents."},{question:"Which component stores vector embeddings in a RAG system?",options:[{text:"Traditional SQL database",isCorrect:!1},{text:"Vector database like Qdrant or Pinecone",isCorrect:!0},{text:"Text file on disk",isCorrect:!1},{text:"Browser localStorage",isCorrect:!1}],explanation:"Vector databases like Qdrant, Pinecone, or Weaviate are specifically designed to store and efficiently search through vector embeddings using similarity metrics."},{question:"What is a common challenge in RAG systems?",options:[{text:"They are too simple to be useful",isCorrect:!1},{text:"Poor chunking can lead to lost context",isCorrect:!0},{text:"They never make mistakes",isCorrect:!1},{text:"They only work with English text",isCorrect:!1}],explanation:"Common RAG challenges include poor chunking losing context, irrelevant retrieval, hallucinations despite context, slow performance, and high costs. Solutions include semantic chunking, reranking, and prompt engineering."},{question:"Why is source citation important in RAG chatbots?",options:[{text:"It makes the response longer",isCorrect:!1},{text:"It provides transparency and allows users to verify information",isCorrect:!0},{text:"It's not important; citations are optional",isCorrect:!1},{text:"It confuses users",isCorrect:!1}],explanation:"Source citations provide transparency, allow users to verify information, build trust, and enable them to explore original documents for deeper understanding. They're essential for RAG system credibility."}],ch8:[{question:"What is the first phase of implementing an AI-driven project?",options:[{text:"Deploy to production immediately",isCorrect:!1},{text:"Requirements gathering and specification writing",isCorrect:!0},{text:"Testing and debugging",isCorrect:!1},{text:"Marketing the product",isCorrect:!1}],explanation:"The first phase is gathering requirements and writing clear specifications. This foundation guides all subsequent AI-assisted development, ensuring the generated code meets actual needs."},{question:"What should you do FIRST when debugging AI-generated code?",options:[{text:"Delete all the code and start over",isCorrect:!1},{text:"Read and understand the generated code",isCorrect:!0},{text:"Immediately ask AI to fix it without investigating",isCorrect:!1},{text:"Deploy to production and hope it works",isCorrect:!1}],explanation:"Always read and understand AI-generated code first. Then use traditional debugging tools, check for common AI mistakes, and refine prompts with specific bug details for fixes."},{question:"Which testing strategy is most important for AI-generated code?",options:[{text:"Skip testing since AI code is always perfect",isCorrect:!1},{text:"Only test manually without automation",isCorrect:!1},{text:"Write tests based on specification acceptance criteria",isCorrect:!0},{text:"Just run the code once and assume it works",isCorrect:!1}],explanation:"Tests should be based on specification acceptance criteria, including unit tests for functions, integration tests for components, edge case testing, and validation against requirements."},{question:"How should AI-generated codebases be maintained over time?",options:[{text:"Never update them; AI code doesn't need maintenance",isCorrect:!1},{text:"Keep specifications updated and refactor regularly",isCorrect:!0},{text:"Delete and regenerate everything monthly",isCorrect:!1},{text:"Only fix critical bugs, ignore everything else",isCorrect:!1}],explanation:"Maintain AI-generated code by keeping specifications current, documenting with human explanations, refactoring regularly, conducting code reviews, and updating as AI models improve."},{question:"What is the role of human oversight in AI-driven development?",options:[{text:"Humans are completely unnecessary",isCorrect:!1},{text:"Humans review, validate, and make critical architectural decisions",isCorrect:!0},{text:"Humans only write documentation",isCorrect:!1},{text:"Humans are only needed for deployment",isCorrect:!1}],explanation:"Human oversight remains crucial for reviewing code, validating against requirements, making architectural decisions, ensuring security, and maintaining code quality. AI assists but doesn't replace human judgment."}]}}}]);